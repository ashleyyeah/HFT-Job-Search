# -*- coding: utf-8 -*-
"""webvscrapping_hft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/196xLeSikdw3NQE9ZW90_6JsFLZMuWRQC
"""

import requests
import bs4
from bs4 import BeautifulSoup
import pandas as pd
import time

import csv

list_of_companies = []

with open('companies.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    for row in csv_reader:
      list_of_companies.append(row[1])
list_of_companies.pop(0)
print(list_of_companies)

# !pip install selenium
# !apt-get update # to update ubuntu to correctly run apt install
# !apt install chromium-chromedriver
# !cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
wd.get("https://www.indeed.com/jobs?q=quantitative%20trader&start=1&vjk=2e2ae18c23945965")

from csv import writer
import csv

file_written = open('jobs.csv', 'w')
writer = csv.writer(file_written)
writer.writerow(["Company Name", "Job Title", "Location","Location-2", "Base Salary", "Job Description"])

# webscrapping for Indeed
def extract_job_title_from_result(soup): 

  for div in soup.find_all(name="div", attrs={"class":"job_seen_beacon"}):
      list_of_features = []
      for a in div.find_all(name="span", attrs={"class":"companyName"}):
        list_of_features.append(a.get_text())
      for a in div.find_all(name="div", attrs={"class":"heading4 color-text-primary singleLineTitle tapItem-gutter"}):
          list_of_features.append(a.get_text())
      for a in div.find_all(name="div", attrs={"class":"companyLocation"}):
        list_of_location = a.get_text().split(",")
        list_of_features.append(list_of_location[0])
        if len(list_of_features) ==2:
          char_index = list_of_location[1].find('+')
          if char_index !=-1:
            word_ = list_of_location[1][0:char_index]
          else:
            word_ = list_of_location[1]
          list_of_features.append(word_)
        else:
          list_of_features.append(" ")
      for a in div.find_all(name="div", attrs={"class":"metadata estimated-salary-container"}):
         list_of_features.append(a.get_text())
      if len(list_of_features) != 5:
         list_of_features.append("N/A")
      for a in div.find_all(name="div", attrs={"class":"job-snippet"}):
         list_of_features.append(a.get_text())
      writer.writerow(list_of_features)
      
# for i in list_of_companies:
#   for i in range(43):
#     URL = "https://www.indeed.com/jobs?q="+ str(i) +"&l&vjk=9011ddd38ff5bc58"
#     #conducting a request of the stated URL above:
#     page = requests.get(URL)
#     #specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.
#     soup = BeautifulSoup(page.text, "html.parser")
#     #printing soup in a more structured tree format that makes for easier reading
#     #print(soup.prettify())
#     extract_job_title_from_result(soup)

for i in range(43):
  URL = "https://www.indeed.com/jobs?q="+ str(i) +"&l&vjk=9011ddd38ff5bc58"
  #conducting a request of the stated URL above:
  page = requests.get(URL)
  #specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.
  soup = BeautifulSoup(page.text, "html.parser")
  #printing soup in a more structured tree format that makes for easier reading
  #print(soup.prettify())
  extract_job_title_from_result(soup)

URL = "https://www.indeed.com/jobs?q=quantitative%20trader&start=1&vjk=2e2ae18c23945965"
#conducting a request of the stated URL above:
page = requests.get(URL)
#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.
soup = BeautifulSoup(page.text, "html.parser")
#printing soup in a more structured tree format that makes for easier reading
#print(soup.prettify())
extract_job_title_from_result(soup)



file_written.close()

# print(job)